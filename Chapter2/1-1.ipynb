{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8058ba",
   "metadata": {},
   "source": [
    "LLMを訓練すための入力テキストを準備\n",
    "=======================\n",
    "テキストをここのワードトークンやサブワードトークンに分割すると、LLM用のベクトル表現にエンコードできる。\n",
    "## トークン化とは？\n",
    "トークン化は、テキストを単語、サブワード、文字などの小さな単位（トークン）に分割するプロセスです。これにより、モデルがテキストを理解し、処理しやすくなります。\n",
    "## トークン化の方法\n",
    "1. **単語トークン化**: テキストをスペースや句読点で区切り、単語ごとに分割します。\n",
    "2. **サブワードトークン化**: 単語をさらに小さな単位に分割します。これにより、未知の単語にも対応しやすくなります。\n",
    "3. **文字トークン化**: テキストを1文字ずつ分割します。これにより、非常に細かいレベルでの解析が可能になります。\n",
    "## トークン化のツール\n",
    "- **NLTK**: Pythonの自然言語処理ライブラリで、トークン化機能を提供しています。\n",
    "- **SpaCy**: 高速で効率的なトークン化をサポートするライブラリです。\n",
    "- **Hugging Face Tokenizers**: 高性能なトークン化ライブラリで、BERTやGPTなどのモデルで使用されます。\n",
    "## トークン化の注意点\n",
    "- トークン化の方法は、使用するモデルやタスクに応じて選択する必要があります。\n",
    "- トークン化の結果は、モデルの性能に大きな影響を与えるため、適切な方法を選ぶことが重要です。\n",
    "## まとめ\n",
    "トークン化は、LLMの訓練において重要なステップです。適切なトークン化方法を選び、テキストを効果的に処理することで、モデルの性能を向上させることができます。\n",
    "\n",
    "バイトペアエンコーディング（BPE）\n",
    "----------------------\n",
    "バイトペアエンコーディング（BPE）は、テキストをトークンに分割するための手法の一つです。BPEは、頻繁に出現する文字のペアを繰り返し結合して、新しいトークンを作成します。これにより、語彙のサイズを制御しつつ、未知の単語にも対応できるようになります。\n",
    "GPT型のLLMで利用されている。\n",
    "\n",
    "LLMをはじめとするディープニューラルネットワークモデルは、Rawなテキストを直接処理できないため、テキストをトークンに分割する必要があります。テキストはカテゴリ刈るデータなので、ニューラルネットワークの実装や訓練に使われる数値計算とは相性が良くない.\n",
    "\n",
    "データをベクトルフォーマットに変換する概念=> 埋め込み（Embedding）\n",
    "特定のニューラルネットワーク層や事前学習済みの別のニューラルネットワークモデルを使って、オーディオ、画像モデルなどの他のデータタイプをベクトルに変換することもできる。\n",
    "埋め込みとは、単語、画像、さらには文章全体といった、離散地のオブジェクトから、連続値のベクトル空間への写像\n",
    "\n",
    "\n",
    "word2vecとは、ニューラルネットワークアーキテクチャを訓練し、目的の単語からその周辺のコンテキストを予測するか、コンテキストの単語群から目的の単語を予測することで、単語の埋め込みを学習する手法です。これにより、単語間の意味的な関係を捉えたベクトル表現が得られます。\n",
    "可視化目的で２次元の単語埋め込みを射影すると、意味的に類似した単語が近くに配置されることがわかります。例えば、「king」と「queen」は「man」と「woman」に対して同じ関係を持つため、これらの単語はベクトル空間でも類似した位置に配置されます。\n",
    "\n",
    "テキストを単語に分割して、単語をトークンに変換し、トークンを埋め込みベクトルに変換する。\n",
    "\n",
    "LLMの訓練のためにトークン化するテキストは、Edith Whartonの小説「The Age of Innocence」の一部です。このテキストは、文学的な内容を含んでおり、LLMが自然言語を理解し生成する能力を向上させるために使用されます。\n",
    "\n",
    "```plaintextThe Age of Innocence by Edith Wharton\n",
    "Copyright 1920 by Edith Wharton\n",
    "Copyright renewed 1948 by Edith Wharton\n",
    "All rights reserved including the right of reproduction in whole or in part in any form.\n",
    "This book is a work of fiction. Names, characters, places and incidents are either the product of the author's imagination or are used fictitiously. Any resemblance to actual persons, living or dead, events, or locales is entirely coincidental.\n",
    "First published in 1920 by D. Appleton and Company, Inc., New York\n",
    "This edition published in 2020 by Public Domain Books\n",
    "www.publicdomainbooks.net\n",
    "```     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "907ca47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b78603e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it'\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:500])  # first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e3d3040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'World!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test-text', ' ', 'with', ' ', 'punctuation.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello World! This is a test-text with punctuation.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd8da241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World! This is a test-text with punctuation.']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'[,.] | \\s' , text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fdfa03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World! This is a test-text with punctuation.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85416b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '!', 'Is', 'this', '--', 'a', 'test?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World! Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\'] |--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e23ab",
   "metadata": {},
   "source": [
    "ここまで実装トークン化スキームは、テキストを個々の単語と句読点文字に分割する。この具体例では、サンプルテキストが10個のトークンに分解される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d11d6b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4286\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\'] |--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(\"Total number of tokens:\", len(preprocessed))\n",
    "print(preprocessed[:30])  # first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c220940",
   "metadata": {},
   "source": [
    "トークンをトークンIDに変換する\n",
    "----------------------\n",
    "Pythonの文字列つから整数表現に変換して、トークンIDを生成する.\n",
    "一意な単語と特殊文字を一意な整数にマッピングする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04ad5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1258\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "529ab717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('\"Ah', 2)\n",
      "('\"Be', 3)\n",
      "('\"Begin', 4)\n",
      "('\"By', 5)\n",
      "('\"Come', 6)\n",
      "('\"Destroyed', 7)\n",
      "('\"Don\\'t', 8)\n",
      "('\"Gisburns', 9)\n",
      "('\"Grindles.\"', 10)\n",
      "('\"Hang', 11)\n",
      "('\"Has', 12)\n",
      "('\"How', 13)\n",
      "('\"I', 14)\n",
      "('\"I\\'d', 15)\n",
      "('\"If', 16)\n",
      "('\"It', 17)\n",
      "('\"It\\'s', 18)\n",
      "('\"Jack', 19)\n",
      "('\"Money\\'s', 20)\n",
      "('\"Moon-dancers', 21)\n",
      "('\"Mr', 22)\n",
      "('\"Mrs', 23)\n",
      "('\"My', 24)\n",
      "('\"Never', 25)\n",
      "('\"Never,', 26)\n",
      "('\"Of', 27)\n",
      "('\"Oh', 28)\n",
      "('\"Once', 29)\n",
      "('\"Only', 30)\n",
      "('\"Or', 31)\n",
      "('\"That', 32)\n",
      "('\"The', 33)\n",
      "('\"Then', 34)\n",
      "('\"There', 35)\n",
      "('\"This', 36)\n",
      "('\"We', 37)\n",
      "('\"Well', 38)\n",
      "('\"What', 39)\n",
      "('\"When', 40)\n",
      "('\"Why', 41)\n",
      "('\"Yes', 42)\n",
      "('\"You', 43)\n",
      "('\"but', 44)\n",
      "('\"deadening', 45)\n",
      "('\"dragged', 46)\n",
      "('\"effects\"', 47)\n",
      "('\"interesting\"', 48)\n",
      "('\"lift', 49)\n",
      "('\"obituary', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b4c93",
   "metadata": {},
   "source": [
    "語彙を使って、新しいテキストをトークンIDに変換すること。\n",
    "LLMの出力を数値からテキストに戻したい場合は、トークンIDを対応するトークンにマッピングする逆の語彙を使う。\n",
    "\n",
    "```plaintext\n",
    "Sample text: \"The Age of Innocence by Edith Wharton\"\n",
    "Tokenized: ['The', 'Age', 'of', 'Innocence', 'by', 'Edith', 'Wharton']\n",
    "Token IDs: [1, 2, 3, 4, 5, 6, 7]\n",
    "``` \n",
    "以下でトークナイザーの実装を行う。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacca45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4b95650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "        if \"UNK\" not in self.str_to_int:\n",
    "            unk_id = len(self.str_to_int)\n",
    "            self.str_to_int[\"UNK\"] = unk_id\n",
    "            self.int_to_str[unk_id] = \"UNK\"\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\'] |--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int.get(s, self.str_to_int[\"UNK\"]) for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "638f41c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [18, 1103, 699, 614, 853, 65, 1251, 692, 1, 962, 123, 67, 1258, 65, 1258, 188, 589, 879, 675, 674, 67, 107, 1230, 1251, 370, 975, 1258]\n",
      "Decoded: \"It's the last he painted, you know,\" said Mr. UNK, UNK a great picture it is. I wish you could see UNK\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab)  # ←インスタンスは小文字で\n",
    "sample_text = \"\"\"\"It's the last he painted, you know,\" said Mr. Poole, \"and a great picture it is. I wish you could see it.\"\"\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(\"Encoded:\", encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8bfc53",
   "metadata": {},
   "source": [
    "特定のコンテキストに対処するために、特別なトークンを追加する。例えば、訓練データセットには含まれず、従って既存の語彙にも含まれていない新しい未知の単語を表すために、<|unk|>トークンを追加することができる。\n",
    "さらに、<|endoftext|> トークンを追加して、無関係な2つのテキストソースを分離できる。\n",
    "無関係なテキストの間にトークンを追加する。例えば、GPT型のLLMに続く各文書や書籍の前にトークンを挿入するのが一般的。このようにすると、「これらのテキストソースは訓練のために連結されているが、実際は無関係である」ことをLLMが理解しやすくなる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406349fc",
   "metadata": {},
   "source": [
    "複数の独立したテキストソースを扱う時には、それらのテキストの間に<|endoftext|>トークンを挿入する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9d0b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(len(vocab.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87a9a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1255)\n",
      "('your', 1256)\n",
      "('yourself', 1257)\n",
      "('<|unk|>', 1258)\n",
      "('<|endoftext|>', 1259)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df7b3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'] |--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87693962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text: Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \"<|endoftext|>\".join([text1, text2])\n",
    "print(\"Combined text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63c990c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1258, 65, 434, 1251, 727, 1258, 1103, 1071, 1098, 825, 1103, 1258]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ede8dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like <|unk|> the sunlit terraces of the <|unk|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632c463",
   "metadata": {},
   "source": [
    "BPE(バイトペアエンコーディング)の実装　<-GPT-2, GPT-3で利用されている。\n",
    " tiktokenというライブラリを使う。tiktokenは、Rustのソースコードに基づいてBPEアルゴリズムを非常効率的に実装したもの。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0663053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp310-cp310-macosx_11_0_arm64.whl (999 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.2/999.2 kB\u001b[0m \u001b[31m961.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /Users/yutokishi/anaconda3/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/yutokishi/anaconda3/lib/python3.10/site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yutokishi/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yutokishi/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yutokishi/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yutokishi/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6dc46b",
   "metadata": {},
   "source": [
    "tiktokenのBPEトークナイザーをインスタンス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd34ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 50256, 818, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokennizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\n",
    "    \"Hello, do you like tea?<|endoftext|>\"\n",
    "    \"In the sunlit terraces of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokennizer.encode(text,allowed_special={\"<|endoftext|>\"})# allowed_special=()で特殊トークンを無効化\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0af789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokennizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2272b98",
   "metadata": {},
   "source": [
    "BPEトークナイザは未知の単語がどのようなものであろうと対処できる。\n",
    "BPEベースとなるアルゴリズムは、事前に定義された5位に含ていない単語を、より小さなサブワードトークンに分割することができる。このようにすると、語彙に存在しない単語でも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "739936ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokennizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\n",
    "    \"Akwirw ier\"\n",
    ")\n",
    "integers = tokennizer.encode(text)\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a35dd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "strings = tokennizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd723cb",
   "metadata": {},
   "source": [
    "BPEは頻出する文字をサブワードにマージし、頻出ささサブワードを単語にマージする。、トークン化できるようになります。例えば、「Akwirw ier」という未知の単語は、「Akw」「ir」「w」「ier」というサブワードに分割され、これらのサブワードは語彙に存在するため、トークン化が可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633b2c5",
   "metadata": {},
   "source": [
    "スライディングウィンドウアプロ使って、入力変数と目的変数のペアをかか訓練デーかrからタからセットから取り出すををデータローダーを実装する。\n",
    "```python\n",
    "import tiktoken\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokennizer.encode(raw_text)\n",
    "print(f\"Length of text in characters: {len(raw_text)}\")\n",
    "print(f\"Length of text in tokens: {len(enc_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fed761a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text in characters: 20479\n",
      "Length of text in tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokennizer.encode(raw_text)\n",
    "print(f\"Length of text in characters: {len(raw_text)}\")\n",
    "print(f\"Length of text in tokens: {len(enc_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0259fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da17fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e2c28",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "次単語予測タスクのための入力変数目的変数のペアを作成する最も簡単な方法は、x,yという二つの変数を。。作成することです。\n",
    "xには入力トークンが含まれ、yには(xをシフトさせたもの)次のトークンが含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7bb4c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size =4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5b02942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [290] the desired output is 4920\n",
      "when input is [290, 4920] the desired output is 2241\n",
      "when input is [290, 4920, 2241] the desired output is 287\n",
      "when input is [290, 4920, 2241, 287] the desired output is 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\"when input is {context} the desired output is {desired}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0dda5dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ->  established\n",
      " and established ->  himself\n",
      " and established himself ->  in\n",
      " and established himself in ->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokennizer.decode(context), \"->\", tokennizer.decode([desired]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceac540",
   "metadata": {},
   "source": [
    "トークンを埋め込みに変換する前に必要な残り作業は、効率的なデータローダを実装することだけ。このデータローダは入力変数と目的変数をPyTorchテンソルのペアとして返す。\n",
    "ここで取得したいのは、入力テンソルとターゲットテンソルという2つのテンソルです。\n",
    "入力テンソルには、LLMがみるテキストが含まれており、ターゲットテンソルには、次に来るトークンが含まれています。\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae1ee107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yutokishi/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11102f85",
   "metadata": {},
   "source": [
    "GPTDatasetV1クラスでは、PytorchのDasetsetクラスを継承し、テキストデータをトークン化して、指定されたコンテキストサイズに基づいて入力とターゲットのペアを生成します。これにより、LLMの訓練に適した形式でデータを提供できます。\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87e4e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力変数と目的変数のペアでバッ生成するデータローダー\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128,shuffle = True, drop_last = True, num_workers = 0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f43aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,\n",
      "           257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,\n",
      "           568,   340,   373,   645,  1049,  5975,   284,   502,   284,  3285,\n",
      "           326,    11,   287,   262,  6001,   286,   465, 13476,    11,   339,\n",
      "           550,  5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,\n",
      "           290,  4920,  2241,   287,   257,  4489,    64,   319,   262, 34686,\n",
      "         41976,    13,   357, 10915,   314,  2138,  1807,   340,   561,   423,\n",
      "           587, 10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,\n",
      "           286,   465, 13476,     1,   438,  5562,   373,   644,   262,  1466,\n",
      "          1444,   340,    13,   314,   460,  3285,  9074,    13, 46606,   536,\n",
      "          5469,   438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,\n",
      "          3255,   465, 48422,   540,   450,    67,  3299,    13,   366,  5189,\n",
      "          1781,   340,   338,  1016,   284,  3758,   262,  1988,   286,   616,\n",
      "          4286,   705,  1014,   510,    26,   475,   314,   836,   470,   892,\n",
      "           286,   326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,\n",
      "           284,   943, 17034,   318,   477,   314,   892,   286,   526,   383,\n",
      "          1573,    11,   319,  9074,    13,   536,  5469,   338, 11914,    11,\n",
      "         33096,   663,  4808,  3808,    62,   355,   996,   484,   547, 12548,\n",
      "           287,   281, 13079,   410, 12523,   286, 22353,    13,   843,   340,\n",
      "           373,   407,   691,   262,  9074,    13,   536, 48819,   508, 25722,\n",
      "           276,    13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,\n",
      "            11,   379,   262,   938,   402,  1617,   261, 12917,   905,    11,\n",
      "          5025,   502,   878,   402,   271, 10899,   338,   366, 31640,    12,\n",
      "            67, 20811,     1,   284,   910,    11,   351, 10953,   287,   607,\n",
      "          2951,    25,   366,  1135,  2236,   407,   804,  2402,   663,   588,\n",
      "           757, 13984,   198,   198,  5779, 28112]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,\n",
      "          7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,   568,\n",
      "           340,   373,   645,  1049,  5975,   284,   502,   284,  3285,   326,\n",
      "            11,   287,   262,  6001,   286,   465, 13476,    11,   339,   550,\n",
      "          5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,   290,\n",
      "          4920,  2241,   287,   257,  4489,    64,   319,   262, 34686, 41976,\n",
      "            13,   357, 10915,   314,  2138,  1807,   340,   561,   423,   587,\n",
      "         10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,   286,\n",
      "           465, 13476,     1,   438,  5562,   373,   644,   262,  1466,  1444,\n",
      "           340,    13,   314,   460,  3285,  9074,    13, 46606,   536,  5469,\n",
      "           438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,  3255,\n",
      "           465, 48422,   540,   450,    67,  3299,    13,   366,  5189,  1781,\n",
      "           340,   338,  1016,   284,  3758,   262,  1988,   286,   616,  4286,\n",
      "           705,  1014,   510,    26,   475,   314,   836,   470,   892,   286,\n",
      "           326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,   284,\n",
      "           943, 17034,   318,   477,   314,   892,   286,   526,   383,  1573,\n",
      "            11,   319,  9074,    13,   536,  5469,   338, 11914,    11, 33096,\n",
      "           663,  4808,  3808,    62,   355,   996,   484,   547, 12548,   287,\n",
      "           281, 13079,   410, 12523,   286, 22353,    13,   843,   340,   373,\n",
      "           407,   691,   262,  9074,    13,   536, 48819,   508, 25722,   276,\n",
      "            13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,    11,\n",
      "           379,   262,   938,   402,  1617,   261, 12917,   905,    11,  5025,\n",
      "           502,   878,   402,   271, 10899,   338,   366, 31640,    12,    67,\n",
      "         20811,     1,   284,   910,    11,   351, 10953,   287,   607,  2951,\n",
      "            25,   366,  1135,  2236,   407,   804,  2402,   663,   588,   757,\n",
      "         13984,   198,   198,  5779, 28112, 10197]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=1,\n",
    "    max_length=256,\n",
    "    stride=1,\n",
    "    shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17e513d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,\n",
      "          7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,   568,\n",
      "           340,   373,   645,  1049,  5975,   284,   502,   284,  3285,   326,\n",
      "            11,   287,   262,  6001,   286,   465, 13476,    11,   339,   550,\n",
      "          5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,   290,\n",
      "          4920,  2241,   287,   257,  4489,    64,   319,   262, 34686, 41976,\n",
      "            13,   357, 10915,   314,  2138,  1807,   340,   561,   423,   587,\n",
      "         10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,   286,\n",
      "           465, 13476,     1,   438,  5562,   373,   644,   262,  1466,  1444,\n",
      "           340,    13,   314,   460,  3285,  9074,    13, 46606,   536,  5469,\n",
      "           438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,  3255,\n",
      "           465, 48422,   540,   450,    67,  3299,    13,   366,  5189,  1781,\n",
      "           340,   338,  1016,   284,  3758,   262,  1988,   286,   616,  4286,\n",
      "           705,  1014,   510,    26,   475,   314,   836,   470,   892,   286,\n",
      "           326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,   284,\n",
      "           943, 17034,   318,   477,   314,   892,   286,   526,   383,  1573,\n",
      "            11,   319,  9074,    13,   536,  5469,   338, 11914,    11, 33096,\n",
      "           663,  4808,  3808,    62,   355,   996,   484,   547, 12548,   287,\n",
      "           281, 13079,   410, 12523,   286, 22353,    13,   843,   340,   373,\n",
      "           407,   691,   262,  9074,    13,   536, 48819,   508, 25722,   276,\n",
      "            13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,    11,\n",
      "           379,   262,   938,   402,  1617,   261, 12917,   905,    11,  5025,\n",
      "           502,   878,   402,   271, 10899,   338,   366, 31640,    12,    67,\n",
      "         20811,     1,   284,   910,    11,   351, 10953,   287,   607,  2951,\n",
      "            25,   366,  1135,  2236,   407,   804,  2402,   663,   588,   757,\n",
      "         13984,   198,   198,  5779, 28112, 10197]]), tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,  7026,\n",
      "         15632,   438,  2016,   257,   922,  5891,  1576,   438,   568,   340,\n",
      "           373,   645,  1049,  5975,   284,   502,   284,  3285,   326,    11,\n",
      "           287,   262,  6001,   286,   465, 13476,    11,   339,   550,  5710,\n",
      "           465, 12036,    11,  6405,   257,  5527, 27075,    11,   290,  4920,\n",
      "          2241,   287,   257,  4489,    64,   319,   262, 34686, 41976,    13,\n",
      "           357, 10915,   314,  2138,  1807,   340,   561,   423,   587, 10598,\n",
      "           393, 28537,  2014,   198,   198,     1,   464,  6001,   286,   465,\n",
      "         13476,     1,   438,  5562,   373,   644,   262,  1466,  1444,   340,\n",
      "            13,   314,   460,  3285,  9074,    13, 46606,   536,  5469,   438,\n",
      "         14363,   938,  4842,  1650,   353,   438,  2934,   489,  3255,   465,\n",
      "         48422,   540,   450,    67,  3299,    13,   366,  5189,  1781,   340,\n",
      "           338,  1016,   284,  3758,   262,  1988,   286,   616,  4286,   705,\n",
      "          1014,   510,    26,   475,   314,   836,   470,   892,   286,   326,\n",
      "            11,  1770,    13,  8759,  2763,   438,  1169,  2994,   284,   943,\n",
      "         17034,   318,   477,   314,   892,   286,   526,   383,  1573,    11,\n",
      "           319,  9074,    13,   536,  5469,   338, 11914,    11, 33096,   663,\n",
      "          4808,  3808,    62,   355,   996,   484,   547, 12548,   287,   281,\n",
      "         13079,   410, 12523,   286, 22353,    13,   843,   340,   373,   407,\n",
      "           691,   262,  9074,    13,   536, 48819,   508, 25722,   276,    13,\n",
      "         11161,   407,   262, 40123, 18113,   544,  9325,   701,    11,   379,\n",
      "           262,   938,   402,  1617,   261, 12917,   905,    11,  5025,   502,\n",
      "           878,   402,   271, 10899,   338,   366, 31640,    12,    67, 20811,\n",
      "             1,   284,   910,    11,   351, 10953,   287,   607,  2951,    25,\n",
      "           366,  1135,  2236,   407,   804,  2402,   663,   588,   757, 13984,\n",
      "           198,   198,  5779, 28112, 10197,   832]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d746d",
   "metadata": {},
   "source": [
    "データセット全ての単語を利用するために、ストライドを４に増やしている。これはバッチ間のオーバーラップを防ぐための措置。オーバーラップが多いと過剰適リスクが高まる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49dd1f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets: tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=8,\n",
    "    max_length=4,\n",
    "    stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Targets:\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b1185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
