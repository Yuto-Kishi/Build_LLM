{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8058ba",
   "metadata": {},
   "source": [
    "LLMを訓練すための入力テキストを準備\n",
    "=======================\n",
    "テキストをここのワードトークンやサブワードトークンに分割すると、LLM用のベクトル表現にエンコードできる。\n",
    "## トークン化とは？\n",
    "トークン化は、テキストを単語、サブワード、文字などの小さな単位（トークン）に分割するプロセスです。これにより、モデルがテキストを理解し、処理しやすくなります。\n",
    "## トークン化の方法\n",
    "1. **単語トークン化**: テキストをスペースや句読点で区切り、単語ごとに分割します。\n",
    "2. **サブワードトークン化**: 単語をさらに小さな単位に分割します。これにより、未知の単語にも対応しやすくなります。\n",
    "3. **文字トークン化**: テキストを1文字ずつ分割します。これにより、非常に細かいレベルでの解析が可能になります。\n",
    "## トークン化のツール\n",
    "- **NLTK**: Pythonの自然言語処理ライブラリで、トークン化機能を提供しています。\n",
    "- **SpaCy**: 高速で効率的なトークン化をサポートするライブラリです。\n",
    "- **Hugging Face Tokenizers**: 高性能なトークン化ライブラリで、BERTやGPTなどのモデルで使用されます。\n",
    "## トークン化の注意点\n",
    "- トークン化の方法は、使用するモデルやタスクに応じて選択する必要があります。\n",
    "- トークン化の結果は、モデルの性能に大きな影響を与えるため、適切な方法を選ぶことが重要です。\n",
    "## まとめ\n",
    "トークン化は、LLMの訓練において重要なステップです。適切なトークン化方法を選び、テキストを効果的に処理することで、モデルの性能を向上させることができます。\n",
    "\n",
    "バイトペアエンコーディング（BPE）\n",
    "----------------------\n",
    "バイトペアエンコーディング（BPE）は、テキストをトークンに分割するための手法の一つです。BPEは、頻繁に出現する文字のペアを繰り返し結合して、新しいトークンを作成します。これにより、語彙のサイズを制御しつつ、未知の単語にも対応できるようになります。\n",
    "GPT型のLLMで利用されている。\n",
    "\n",
    "LLMをはじめとするディープニューラルネットワークモデルは、Rawなテキストを直接処理できないため、テキストをトークンに分割する必要があります。テキストはカテゴリ刈るデータなので、ニューラルネットワークの実装や訓練に使われる数値計算とは相性が良くない.\n",
    "\n",
    "データをベクトルフォーマットに変換する概念=> 埋め込み（Embedding）\n",
    "特定のニューラルネットワーク層や事前学習済みの別のニューラルネットワークモデルを使って、オーディオ、画像モデルなどの他のデータタイプをベクトルに変換することもできる。\n",
    "埋め込みとは、単語、画像、さらには文章全体といった、離散地のオブジェクトから、連続値のベクトル空間への写像\n",
    "\n",
    "\n",
    "word2vecとは、ニューラルネットワークアーキテクチャを訓練し、目的の単語からその周辺のコンテキストを予測するか、コンテキストの単語群から目的の単語を予測することで、単語の埋め込みを学習する手法です。これにより、単語間の意味的な関係を捉えたベクトル表現が得られます。\n",
    "可視化目的で２次元の単語埋め込みを射影すると、意味的に類似した単語が近くに配置されることがわかります。例えば、「king」と「queen」は「man」と「woman」に対して同じ関係を持つため、これらの単語はベクトル空間でも類似した位置に配置されます。\n",
    "\n",
    "テキストを単語に分割して、単語をトークンに変換し、トークンを埋め込みベクトルに変換する。\n",
    "\n",
    "LLMの訓練のためにトークン化するテキストは、Edith Whartonの小説「The Age of Innocence」の一部です。このテキストは、文学的な内容を含んでおり、LLMが自然言語を理解し生成する能力を向上させるために使用されます。\n",
    "\n",
    "```plaintextThe Age of Innocence by Edith Wharton\n",
    "Copyright 1920 by Edith Wharton\n",
    "Copyright renewed 1948 by Edith Wharton\n",
    "All rights reserved including the right of reproduction in whole or in part in any form.\n",
    "This book is a work of fiction. Names, characters, places and incidents are either the product of the author's imagination or are used fictitiously. Any resemblance to actual persons, living or dead, events, or locales is entirely coincidental.\n",
    "First published in 1920 by D. Appleton and Company, Inc., New York\n",
    "This edition published in 2020 by Public Domain Books\n",
    "www.publicdomainbooks.net\n",
    "```     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "907ca47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b78603e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it'\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:500])  # first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e3d3040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'World!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test-text', ' ', 'with', ' ', 'punctuation.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello World! This is a test-text with punctuation.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd8da241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World! This is a test-text with punctuation.']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'[,.] | \\s' , text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fdfa03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World! This is a test-text with punctuation.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85416b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '!', 'Is', 'this', '--', 'a', 'test?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World! Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\'] |--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e23ab",
   "metadata": {},
   "source": [
    "ここまで実装トークン化スキームは、テキストを個々の単語と句読点文字に分割する。この具体例では、サンプルテキストが10個のトークンに分解される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d11d6b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4286\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\'] |--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(\"Total number of tokens:\", len(preprocessed))\n",
    "print(preprocessed[:30])  # first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c220940",
   "metadata": {},
   "source": [
    "トークンをトークンIDに変換する\n",
    "----------------------\n",
    "Pythonの文字列つから整数表現に変換して、トークンIDを生成する.\n",
    "一意な単語と特殊文字を一意な整数にマッピングする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04ad5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1258\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "529ab717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('\"Ah', 2)\n",
      "('\"Be', 3)\n",
      "('\"Begin', 4)\n",
      "('\"By', 5)\n",
      "('\"Come', 6)\n",
      "('\"Destroyed', 7)\n",
      "('\"Don\\'t', 8)\n",
      "('\"Gisburns', 9)\n",
      "('\"Grindles.\"', 10)\n",
      "('\"Hang', 11)\n",
      "('\"Has', 12)\n",
      "('\"How', 13)\n",
      "('\"I', 14)\n",
      "('\"I\\'d', 15)\n",
      "('\"If', 16)\n",
      "('\"It', 17)\n",
      "('\"It\\'s', 18)\n",
      "('\"Jack', 19)\n",
      "('\"Money\\'s', 20)\n",
      "('\"Moon-dancers', 21)\n",
      "('\"Mr', 22)\n",
      "('\"Mrs', 23)\n",
      "('\"My', 24)\n",
      "('\"Never', 25)\n",
      "('\"Never,', 26)\n",
      "('\"Of', 27)\n",
      "('\"Oh', 28)\n",
      "('\"Once', 29)\n",
      "('\"Only', 30)\n",
      "('\"Or', 31)\n",
      "('\"That', 32)\n",
      "('\"The', 33)\n",
      "('\"Then', 34)\n",
      "('\"There', 35)\n",
      "('\"This', 36)\n",
      "('\"We', 37)\n",
      "('\"Well', 38)\n",
      "('\"What', 39)\n",
      "('\"When', 40)\n",
      "('\"Why', 41)\n",
      "('\"Yes', 42)\n",
      "('\"You', 43)\n",
      "('\"but', 44)\n",
      "('\"deadening', 45)\n",
      "('\"dragged', 46)\n",
      "('\"effects\"', 47)\n",
      "('\"interesting\"', 48)\n",
      "('\"lift', 49)\n",
      "('\"obituary', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b4c93",
   "metadata": {},
   "source": [
    "語彙を使って、新しいテキストをトークンIDに変換すること。\n",
    "LLMの出力を数値からテキストに戻したい場合は、トークンIDを対応するトークンにマッピングする逆の語彙を使う。\n",
    "\n",
    "```plaintext\n",
    "Sample text: \"The Age of Innocence by Edith Wharton\"\n",
    "Tokenized: ['The', 'Age', 'of', 'Innocence', 'by', 'Edith', 'Wharton']\n",
    "Token IDs: [1, 2, 3, 4, 5, 6, 7]\n",
    "``` \n",
    "以下でトークナイザーの実装を行う。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacca45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4b95650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "        if \"UNK\" not in self.str_to_int:\n",
    "            unk_id = len(self.str_to_int)\n",
    "            self.str_to_int[\"UNK\"] = unk_id\n",
    "            self.int_to_str[unk_id] = \"UNK\"\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\'] |--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int.get(s, self.str_to_int[\"UNK\"]) for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "638f41c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [18, 1103, 699, 614, 853, 65, 1251, 692, 1, 962, 123, 67, 1258, 65, 1258, 188, 589, 879, 675, 674, 67, 107, 1230, 1251, 370, 975, 1258]\n",
      "Decoded: \"It's the last he painted, you know,\" said Mr. UNK, UNK a great picture it is. I wish you could see UNK\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab)  # ←インスタンスは小文字で\n",
    "sample_text = \"\"\"\"It's the last he painted, you know,\" said Mr. Poole, \"and a great picture it is. I wish you could see it.\"\"\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(\"Encoded:\", encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8bfc53",
   "metadata": {},
   "source": [
    "特定のコンテキストに対処するために、特別なトークンを追加する。例えば、訓練データセットには含まれず、従って既存の語彙にも含まれていない新しい未知の単語を表すために、<|unk|>トークンを追加することができる。\n",
    "さらに、<|endoftext|> トークンを追加して、無関係な2つのテキストソースを分離できる。\n",
    "無関係なテキストの間にトークンを追加する。例えば、GPT型のLLMに続く各文書や書籍の前にトークンを挿入するのが一般的。このようにすると、「これらのテキストソースは訓練のために連結されているが、実際は無関係である」ことをLLMが理解しやすくなる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406349fc",
   "metadata": {},
   "source": [
    "複数の独立したテキストソースを扱う時には、それらのテキストの間に<|endoftext|>トークンを挿入する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9d0b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(len(vocab.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87a9a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1255)\n",
      "('your', 1256)\n",
      "('yourself', 1257)\n",
      "('<|unk|>', 1258)\n",
      "('<|endoftext|>', 1259)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'] |--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
